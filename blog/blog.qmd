---
title: "Analisis de accidentes viales en la ciudad de Medellin"
lang: es    
bibliography: references.bib
author:
  - name: Daniel Daza Macias
    email: dadazam@unal.edu.co
  - name: Daniel Santiago Cadavid Montoya
    email: dcadavid@unal.edu.co
  - name: Jose Daniel Bustamante Arango
    email: jobustamantea@unal.edu.co
  - name: Marlon Calle Areiza
    email: mcallea@unal.edu.co
  - name: Ronald Gabriel Palencia
    email: ropalencia@unal.edu.co
format:
  html:
    code-fold: true
jupyter: python3
echo: false
theme:
          light: flatly
          dark: darkly
toc: true
appendix-style: default
---

# Introducción

El siguiente trabajo tiene como objetivo la predicción de accidentalidad en la ciudad de Medellín a partir de la historia reciente de accidentes reportados y el analisis de accidentalidad de los barrios de la ciudad por grupos.

Para realizar el siguiente reporte utilizamos la base de datos facilitada por @DataWebsite que además contiene un diccionario para cada una de las columnas.

## Contexto del problema

Uno de los problemas principales que tienen todas las ciudades del mundo son los incidentes de transitos. Se entiende por Incidente de tránsito: evento, generalmente involuntario, generado al menos por un vehículo en movimiento, que causa daños a personas y bienes involucrados en él, e igualmente afecta la normal circulación de los vehículos que se movilizan por la vía o vías comprendidas en el lugar o dentro de la zona de influencia del hecho [@eu03].

Medellín como ciudad que aumenta su parque automotor cada año requiere medidas para lidiar con dichos incidentes, es por esto que se propone para la ciudad y sus ciudadanos la predicción de accidentalidad (cantidad de accidentes) dada una cierta fecha, además de un análisis por grupos de barrios con características similares para así tomar decisiones en el ámbito de la política pública que permitan las mejores soluciones para dicho problema. 


## Metodos empleados y objetivos de desarrollo

Teniendo en cuenta el contexto anterior en el siguiente trabajo se desarollará un modelo predictivo, basados en técnicas de aprendizaje estadístico, para así obtener la accidentalidad de una de cinco clases de accidente, entre las cuales encontramos: 'choque', 'atropello' 'volcamiento' 'caida de ocupante' 'incendio' y 'otro'. En este caso nos enfocaremos en el tipo 'atropello', esto con el objetivo de ofrecer un producto enfocado en el peatón, principal implicado de este tipo de accidente y el más vulnerable y prioritario en temas generales de movilidad.

Además de ello se planea ofrecer un análisis sobre grupos de barrios, esto con el objetivo de generar un producto suficiente informativo tanto para ciudanos comunes que deséen tomar decisiones que impliquen movilidad general como para entes gubernamentales que planéen implementar políticas en pro de la solución del problema mencionado anteriormente. 

Se usará Google colab y Python junto con las librerías sklearn y statsmodels, además del framework streamlit para la creación de un aplicativo web que permita mirar diferentes informaciones generadas en el área, sea la predicción o la agrupación.

## Resumen resultados

Utilizando un modelo lineal generalizado de tipo poisson para la predicción de accidentes obtenemos valores reales que mapeados a enteros van desde 8 atropellos a 12 atropellos. Por otra parte a la hora de agrupar barrios se encontraron grupos de barrios con poca cantidad de barrios que además tenían la mayor accidentalidad, se recomendó así poner especial atención a estos.

# Importe y análisis de datos

Para comenzar se hará un cargue del conjunto de datos:

```{python}
#| tbl-cap: Datos iniciales
#| label: tbl-import-presentacion-inicial
#| warning: false

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
#from google.colab import drive
import numpy as np
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency
%matplotlib inline
#import plotnine as p9
#import holidays_co
from datar.dplyr import group_by, summarise, n
from datetime import datetime
from IPython.display import Markdown
from tabulate import tabulate

pd.set_option("display.max_columns", 4)


df = pd.read_csv('incidentes_viales.csv', sep = ";")
holidays = pd.read_csv("holidays.csv")

df.head(5)
```

En @tbl-import-presentacion-inicial se puede observar el dataframe inicial de los datos, donde además se puede apreciar el tipo de dato de algunas de las columnas.

Por otra parte, como se ve en @tbl-import-dias-festivos  también se hace un cargue de datos a un dataset que contiene días festivos que será usado en siguientes secciones.


```{python}
#| tbl-cap: Dias festivos
#| label: tbl-import-dias-festivos 

pd.set_option("display.max_columns", 15)

# limpiar los nombres de la columnas 
df.columns=["ano",	"cbml",	"clase_accidente",	"direccion",	"direccion_encasillada",	"diseno","expediente",	"fecha_accidente",	"fecha_accidentes",	"gravedad_accidente",	"mes",	"nro_radicado",	"numcomuna",	"barrio",	"comuna",	"location",	"x",	"y"]

holidays
```

Antes de continuar se hará un cambio de nombre a algunas de las columnas para simplificar la escritura de las mismas. En @tbl-descripcion-variables se puede apreciar la descripción y el tipo de dato para cada una de las variables.

```{python}
#| tbl-cap: Descripción variables
#| label: tbl-descripcion-variables 

descr_var={"ano":			"Año de ocurrencia del incidente",
"cbml":			"Codigo catastral que corresponde al codigo comuna, barrio, manzana, lote catastral de un predio.",
"clase_accidente":	"Clasificación del IPAT sobre la clase de accidente de transito: choque, atropello, volcamiento, caida de ocupante, incendio, u otro (que no corresponde a las anteriores 5 clasificaciones, p. ej: sumersión)",
"direccion":		"Dirección donde ocurrió el incidente",
"direccion_encasillada":"Dirección encasillada que entrega el geocodificador",
"diseno":		"Sitio de la vía donde ocurrió el accidente: Cicloruta, Glorieta, Interseccion, Lote o Predio, Paso a Nivel, Paso Elevado, Paso Inferior, Pontón, Puente, Tramo de via, Tunel, Via peatonal",
"expediente":		"Consecutivo que asigna UNE, según el orden de llegada de los expedientes para su diligenciamiento",
"fecha_accidente":	"Fecha del accidente, proviene del IPAT - Informe Policial de accidente de Tránsito",
"fecha_accidentes":	"Fecha de los accidente (formato YYYY-MM-DD hh:mi:ss), proviene del IPAT - Informe Policial de accidentes de Tránsito",
"gravedad_accidente":	"Clasificación del IPAT - Informe Policial de Accidentes de Tránsito, sobre la gravedad del accidente, corresponde al resultado más grave presentado en el accidente. Daños materiales \"Sólo daños\", accidente con heridos \"Herido\", accidente con muertos \"Muerto\". No indica cantidad",
"mes":			"Mes de ocurrencia del incidente vial",
"nro_radicado":		"Consecutivo que asigna UNE, según el orden de llegada de los expedientes para su diligenciamiento",
"numcomuna":		"Numero de la comununa en la que ocurrio incidente vial",
"barrio":		"Barrio de ocurrencia del incidente vial",
"comuna":		"Denominación con la cual se identifica cada Comuna o Corregimiento. 01:Popular 02:Santa Cruz 03:Manrique 04:Aranjuez 05:Castilla 06:Doce de Octubre 07:Robledo 08:Villa Hermosa 09:Buenos Aires 10:La Candelaria 11:Laureles - Estadio 12:La América 13:San Javier 14:El Poblado 15:Guayabal 16:Belén 50:San Sebastián de Palmitas 60:San Cristobal 70:Altavista 80:San Antonio de Prado 90:Santa Elena 99:Toda la Ciudad",
"location":		"Fuente de información con la cual se realizó la geocodificación",
"x":			"Coordenada X en metros del accidente, en sistema de coordenadas MAGNA Medellin Local",
"y":			"Coordenada Y en metros del accidente, en sistema de coordenadas MAGNA Medellin Local"}



table = [[i, df[i].dtype,j ] for i,j in descr_var.items()]

Markdown(tabulate(
  table, 
  headers=["Variable","Tipo de dato", "Descripción"]
))
```

Como se puede observar en @tbl-anal-nulos nuestro dataset contiene una cantidad de datos nulos, que para las variables que nos interesa es bastante mínima por lo que se procederán a eliminar. 

```{python}
#| tbl-cap: Análisis de datos nulos
#| label: tbl-anal-nulos 

def utiles_por_porcentaje(df, columnas, porcentaje):
  new_df = df[columnas]
  total = len(new_df)
  datos = {'columna': [], 'porcentaje_datos_nulos': [], 'datos_nulos': []}
  for col in new_df.columns:
    nulos = new_df[col].isna().sum()
    datos['columna'].append(col)
    datos['porcentaje_datos_nulos'].append(nulos/total)
    datos['datos_nulos'].append(f'{nulos} de {total}')
  nulos_columnas = pd.DataFrame(datos)
  mayor_02 = nulos_columnas['porcentaje_datos_nulos'] <= porcentaje
  utiles = nulos_columnas[mayor_02].sort_values(by='porcentaje_datos_nulos')
  return utiles


utiles_por_porcentaje(df, df.columns, 0.1)
```

Además dada la tabla de descripciones identificamos como variables de poco interés a: 'cbml', 'direccion', 'direccion_encasillada', 'expediente', 'nro_radicado' y 'location', esto debido al poco valor aportante identificado en estas.

```{python}
#| warning: false



## eliminación de valores nulos, esto se debe discutir 
df_con_na = df.copy()
df=df_con_na.dropna()

tildes = {'\\xC1': 'Á',
'\\xE1': 'á',
'\\xE9': 'é',
'\\xED': 'í',
'\\xF3': 'ó',
'\\xF1': 'ñ',
'\\xFA': 'ú',
'\xC1': 'Á',
'\xE1': 'á',
'\xE9': 'é',
'\xED': 'í',
'\xF3': 'ó',
'\xF1': 'ñ',
'\xFA': 'ú'
}


#df['barrio'] = df['barrio'].apply(lambda x:x.replace('\\xC1','Á'))
for col in ['clase_accidente','direccion','diseno','gravedad_accidente','barrio','comuna']:
  for word, replacement in tildes.items():
    df[col] = df[col].apply(lambda x: x.replace(word,replacement))

```

## Descripción de los datos

Como se pudo notar en @tbl-descripcion-variables la mayoría de las variables son categóricas. Así, se procederá a ver los valores únicos de cada una.

```{python}
#| tbl-cap: Cantidad de datos únicos en clase_accidente V0
#| label: tbl-count-clase_accidente-0 



table = [[i, j] for i,j in df.clase_accidente.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Cantidad de datos"]
))
```

Como se puede notar en @tbl-count-clase_accidente-0 existen dos valores para: relacionados a la caída de un ocupante, a pesar de que en @tbl-descripcion-variables sólo se habla de uno, por lo que se procederá a juntar estos dos valores: 

```{python}
#| tbl-cap: Cantidad de datos únicos en clase_accidente V1
#| label: tbl-count-clase_accidente-1


df=df.replace(["Caída de Ocupante"],"Caida Ocupante") 

table = [[i, j] for i,j in df.clase_accidente.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Cantidad de datos"]
))
```

Como se puede notar en @tbl-count-clase_accidente-1 una gran cantidad de los datos corresponden al valor 'Choque', mientras que por otro lado el valor 'incendio' es quien menos valores tiene. 

```{python}
#| tbl-cap: Cantidad de datos únicos en diseno
#| label: tbl-count-diseno 



table = [[i, j] for i,j in df.diseno.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Cantidad de datos"]
))
```

Se puede notar en @tbl-count-diseno que una gran cantidad de los accidentes ocurrieron en tramo de via, mientras que en vía peatonales, túneles y pontones es donde menos ocurrieron.


```{python}
#| tbl-cap: Cantidad de datos únicos en gravedad_accidente
#| label: tbl-count-gravedad_accidente 



table = [[i, j] for i,j in df.gravedad_accidente.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Cantidad de datos"]
))
```

Se puede notar en @tbl-count-gravedad_accidente que una poca cantidad de accidentes ocurren con muertos.

```{python}
#| tbl-cap: Cantidad de datos únicos en mes
#| label: tbl-count-mes 



table = [[i, j] for i,j in df.mes.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Cantidad de datos"]
))
```

Además a primera vista se puede notar en @tbl-count-mes que existe una uniformidad sobre el mes en el que ocurren los accidentes.

```{python}
#| tbl-cap: Cantidad de datos únicos en comuna V0
#| label: tbl-count-comuna-0



table = [[i, j] for i,j in df.comuna.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Cantidad de datos"]
))
```

De @tbl-count-comuna-0 se puede notar que además de los 21 posibles valores que se mencionan en @tbl-descripcion-variables (5 corregimiento y 16 comunas) existen 4 otros posibles valores:'0', 'In', 'AU' y 'SN' de los cuales no se tienen conocimiento algunos. Ya que en caso de utilizar la variable 'comuna' sólo necesitaremos los variables mencionados en la tabla de descripción de variables se procederá a eliminar observaciones con dichos valores:

```{python}
#| tbl-cap: Cantidad de datos únicos en comuna V1
#| label: tbl-count-comuna-1

df.drop(df[df['comuna'] == "0"].index, inplace = True)
df.drop(df[df['comuna'] == "In"].index, inplace = True)
df.drop(df[df['comuna'] == "AU"].index, inplace = True)
df.drop(df[df['comuna'] == "SN"].index, inplace = True)

table = [[i, j] for i,j in df.comuna.value_counts().items()]

Markdown(tabulate(
  table, 
  headers=["Valor", "Cantidad de datos"]
))
```

De @tbl-count-comuna-1 se puede notar que la candelaria es la comuna en la que más ocurren accidentes, mientras que es común que los corregimientos estén bajos en cantidades.


# Formulación modelo predictivo

Para la formulación del modelo se tiene en cuenta que se espera predecir la cantidad de accidentes dado un día de la semana (Lunes, Martes, ..., Domingo), mes y semana del mes contando meses en los que habrán 4 semanas y otros en los que habrán 5 (cambiando dependiendo de qué día comience la semana), considerando además si el día es festivo y si es quincena,  tomando como quincena los días 15, 30 y 31.

Así, se propone un modelo de regresión lineal tipo poisson que se recomienda para cuando la variable a predecir funciona como o es un conteo de eventos [@EcoAnal]. 

## Preparación datos

Partiendo del dataframe y los cambios hechos anteriormente comenzaremos contando las observaciones de accidentes de tipo 'Atropello' para cada día:

```{python}
#| tbl-cap: Dataframe resultante inicial
#| label: tbl-nuevo-df 
#| warning: false

#df.columns
df['fecha_accidente']=pd.to_datetime(df['fecha_accidente'], format='%d/%m/%Y %H:%M:%S')

summarise.inform = False

conteos = (
     df >>
     group_by(df.fecha_accidente , df.clase_accidente) >>
     summarise(y=n())

 )



conteos = pd.DataFrame(conteos)

conteos=conteos[conteos["clase_accidente"]=="Atropello"]
conteos["fecha"] = conteos.fecha_accidente.dt.date
conteos['fecha']=pd.to_datetime(conteos['fecha'], format='%Y/%m/%d')

conteos = conteos.groupby(["fecha"]).sum().reset_index()

conteos
```

Para cada observación resultante las variables mencionadas anteriormente usando la fecha de la observación como base, además del año para hacer la división en datos de entrenamiento y validación. Se continuará agregando la variable binaria 'dia_especial' indicando si el día en cuestión es festivo cruzando cada fecha con el dataframe @tbl-import-dias-festivos obtenido de la librería holidays_co. Y se verá si el día es 15, 30 0 31 para la variable binaria 'quincena'.

```{python}
#| tbl-cap: Dataframe resultante
#| label: tbl-nuevo-df-1


conteos["anio"] = conteos.fecha.dt.year
conteos["dia"] = conteos.fecha.dt.day_name()
conteos["mes"] = conteos.fecha.dt.month
conteos['semana_del_mes'] = conteos['fecha'].apply(lambda d: (d.day-1) // 7 + 1)

conteos['dia_especial'] = np.where(conteos.loc[:,'fecha'].isin(holidays['holidays_fecha']),1,0)
conteos['quincena'] = np.where(conteos.loc[:,'fecha'].dt.day.isin([15,30,31]),1,0)


conteos
```

### Revisión de Outliers

Para revisar outliers y permitir generalización del modelo se usará el Z-Score o número de desviaciones estándares por arriba o abajo de la media. 

En @tbl-df-outliers-1-7 se pueden observar aquellos valores que superan 1.7 desviaciones estándares de su media. Se procederá eliminando estos datos terminando así con el 90.1087% de la variabilidad y 2047 de 2242 datos.

```{python}
#| tbl-cap: Outliers por encima de 1.7
#| label: tbl-df-outliers-1-7

from scipy import stats
import numpy as np

z = np.abs(stats.zscore(conteos["y"]))

conteos["Z"]=z
conteos[conteos["Z"]>1.7]
```


### División en Train y Validation

Así y luego de divididos los datos en train y validation terminamos con dos dataframes @tbl-df-train y @tbl-df-validation respectivamente.

```{python}
#| tbl-cap: Dataframe de entrenamiento
#| label: tbl-df-train
#| warning: false


conteos=conteos[conteos["Z"]<=1.7]

conteos['mes'] = conteos['mes'].astype('str')
conteos['dia_especial'] = conteos['dia_especial'].astype('str')
conteos['semana_del_mes'] = conteos['semana_del_mes'].astype('str')
conteos['quincena'] = conteos['quincena'].astype('str')





train = conteos[conteos["anio"] <= 2018].copy()

train.drop(['anio', 'fecha'], axis = 1, inplace = True)

test = conteos[conteos["anio"] >= 2019].copy()

test.drop(['anio', 'fecha'], axis = 1, inplace = True)

train
```

```{python}
#| tbl-cap: Dataframe de validacion
#| label: tbl-df-validation



test
```

## Entrenamiento del modelo y revisión de métricas

Luego de divididos los datos pasamos a entrenar el modelo para el que obtendremos un conjunto de coeficientes, cuya información se puede encontrar en @tbl-summary-0.

```{python}
#| tbl-cap: Resumen resultados
#| label: tbl-summary-0

formula = 'y~ dia+mes+semana_del_mes+dia_especial+quincena'

import statsmodels.formula.api as smf
import statsmodels.api as sm


model = smf.glm(formula = formula, data=train, family=sm.families.Poisson()).fit()


#Lectura de summary_1

summary_1 = pd.read_csv('resultados_summary.csv', sep = ",")

cabeza=["Item 1", "Descripcion 1","Item 2" ,"Descripcion 2"]
summary_1 = pd.read_csv('resultados_summary.csv', sep = ",", header=None)

Markdown(tabulate(
  summary_1.drop(8), 
  headers=summary_1.columns
))
```

Se puede notar de @tbl-summary-0 que dado el P>|z| podemos concluir como significativas gran parte de las variables, a excepción de: si un día es sábado, si un día es miércoles, si el mes de la fecha es noviembre, si el mes de la fecha es junio, si un día es quincena o no, si hace parte de la semana cinco, cuatro, tres o dos del mes, si es un dia especial o no e incluso.

```{python}
#| tbl-cap: Resultados MAE
#| label: tbl-mae


predict_tr = model.predict(train)
mae_train = np.mean(abs(train.y - predict_tr))


predict = model.predict(test)
mae_test = np.mean(abs(test.y - predict))


Diferencia_porcentual = str((1-mae_train/mae_test)*100) + "%"

tabla=[
  ["MAE entrenamiento",mae_train],
  ["MAE validación",mae_test],
  ["Diferencia porcentual", Diferencia_porcentual]
]

Markdown(tabulate(
  tabla, 
  headers=["Dato","Valor"]
))

```

De @tbl-mae podemos notar que tenemos un MAE para los datos de entrenamiento de 	2.5627 mientras que para los datos de validación es de 	2.8615, comparando los dos resultados se puede notar que existe una diferencia porcentual de 10.4428% entre estos por lo que podemos no confirmar la existencia de sobreajuste. 


## Analisis resultados

Entrenado y validado el modelo con los datos de los años del 2014 al 2020 pasaremos a revisar cómo se comporta este para los años 2021 y 2022, para ello crearemos un dataset de testeo que partirá de la fechas 1/1/2021 e ira hasta 31/12/2022, donde agregaremos las variables usadas para el entrenamiento para cada observación generada en este rango de tiempo:

```{python}
#| tbl-cap: Dataframe de testeo
#| label: tbl-df-test


from pandas.core import apply
from datetime import datetime, date, timedelta
import holidays_co 

inicio = datetime(2021,1,1)
fin    = datetime(2022,12,31)

lista_fechas = [inicio + timedelta(days=d) for d in range((fin - inicio).days + 1)] 


validacion = pd.DataFrame(lista_fechas, columns=['fecha'])

validacion["anio"] = validacion.fecha.dt.year
validacion["dia"] = validacion.fecha.dt.day_name()
validacion["mes"] = validacion.fecha.dt.month
validacion['semana_del_mes'] = validacion['fecha'].apply(lambda d: (d.day-1) // 7 + 1)
validacion['quincena'] = np.where(validacion.loc[:,'fecha'].dt.day.isin([15,30,31]),1,0)
validacion['dia_especial'] = np.where(validacion["fecha"].apply(lambda x: holidays_co.is_holiday_date(x)),1,0)


validacion.head()
```

Así partiendo de @tbl-df-test obtendremos las predicciones iniciales encontradas en la columna 'prediccion' de @tbl-df-test-pred que además se pueden observar en @fig-st-test.

```{python}
#| tbl-cap: Dataframe de testeo con predicciones
#| label: tbl-df-test-pred

validacion['mes'] = validacion['mes'].astype('str')
validacion['dia_especial'] = validacion['dia_especial'].astype('str')
validacion['semana_del_mes'] = validacion['semana_del_mes'].astype('str')
validacion['quincena'] = validacion['quincena'].astype('str')

pred_test = model.predict(validacion)
validacion["prediccion"]=pred_test

# si la aplico la ceil la grafica de ve fea, sin embargo el valor debe ser entero
#validacion["prediccion"] = validacion["prediccion"].apply(np.ceil)

validacion.head()
```

```{python}
#| fig-cap: Prediccion inicial cantidad de accidentes 2021-2022
#| label: fig-st-test

import plotly.express as px

fig = px.line(validacion, x='fecha', y="prediccion",
              title="Predicción de cantidad de accidentes tipo atropello en Medellín 2021 - 2022")
fig.show()
```

De aquellos pronósticos iniciales de @fig-st-test se puede decir que dan como resultado una serie con cambios de nivel y pendiente nula, de varianza aproximadamente constante donde hay indicios de presencia de un patrón periódico siendo los menores valores pronosticados para el mes de enero y los mayores para el mes de septiembre.

Dichos resultados iniciales son mejores de entender en el conjunto de los enteros pues, por ejemplo, es complicado hablar de 4.5 atropellos en un día, por lo que se hace necesario mapear el conjunto de resultados que en un principio es subconjunto de los reales a dicho conjunto ordenado. La función escogida para este mapeo será el piso. Así, transformando cada uno de estos valores iniciales podemos obtener las siguientes estadísticas descriptivas sobre las predicciones para los años:

```{python}
#| tbl-cap: Estadisticas descriptivas predicciones enteras
#| label: tbl-sum-test-pred-int

import math


validacion["prediccion_entera"] =validacion["prediccion"].apply(np.floor)

sum_val_resul=validacion.prediccion_entera.describe()


tabla=[[i,j] for i, j in sum_val_resul.items()]

Markdown(tabulate(
  tabla, 
  headers=["Dato","Valor"]
))

```

De @tbl-sum-test-pred-int se puede notar que la media para la cantidad de atropellos en los dos años es de 9.97 para una desviación estándar de 0.793

Además como se puede observar en el histograma @fig-hist-test los valores de atropellos por día van de 8 a 12:

![Histograma de predicciones enteras](histograma_2021_2022.png){#fig-hist-test}

Por otra parte al graficar dichos valores enteros en el espacio de tiempo obtenemos @fig-st-test-int:

```{python}
#| fig-cap: Prediccion cantidad de accidentes 2021-2022
#| label: fig-st-test-int

fig = px.line(validacion, x='fecha', y="prediccion_entera",
              title="Predicción de cantidad de accidentes tipo atropello en Medellín 2021 - 2022")
fig.show()
```

# Caracterización por barrios

De acuerdo a los objetivos planteados desde un principio se hace interesante analizar el comportamiento de los barrios de acuerdo a diferentes variables.

## Creación de variables

En primera instancia crearemos un dataframe que por barrio cuente la cantidad de accidentes con muertos, con heridos y sólo daños:


```{python}
#| tbl-cap: Creación de acuerdo a gravedad_accidente
#| label: tbl-cre-gravedad_accidente


x = df[['barrio','gravedad_accidente']].value_counts()

nuevo_df = pd.DataFrame(x)
nuevo_df = nuevo_df.reset_index()

nuevo_df = nuevo_df.pivot(index='barrio', columns='gravedad_accidente', values=0).reset_index()
nuevo_df = nuevo_df.fillna(0)


nuevo_df
```

Se continuará creando una cuenta por barrio de tipos de accidentes, diferenciando por: Caida de Ocupante, Choque, Incendio, Otro, y Volcamiento:

```{python}
#| tbl-cap: Creación de acuerdo a clase_accidente
#| label: tbl-cre-clase_accidente

x1 = df[['barrio','clase_accidente']].value_counts()
x1

nuevo_df1 = pd.DataFrame(x1)
nuevo_df1 = nuevo_df1.reset_index()

nuevo_df1 = nuevo_df1.pivot(index='barrio', columns='clase_accidente', values=0).reset_index()
nuevo_df1 = nuevo_df1.fillna(0)

#nuevo_df1 = nuevo_df1[['barrio','Atropello']]
nuevo_df1

```

Se continuará creando una cuenta por barrio de lugar donde ocurrieron los accidentes:

```{python}
#| tbl-cap: Creación de acuerdo a diseno
#| label: tbl-cre-diseno

diseño = df[['barrio','diseno']].value_counts()
df_diseño = pd.DataFrame(diseño)
df_diseño = diseño.reset_index()
df_diseño = df_diseño.pivot(index='barrio', columns='diseno', values=0).reset_index()
df_diseño = df_diseño.fillna(0)
df_diseño

```

Así y haciendo un merge terminaríamos con un dataframe de 22 columnas y 427 filas como se puede ver en @tbl-barrios-1.

```{python}
#| tbl-cap: Dataframe barrios antes de limpieza
#| label: tbl-barrios-1
juntos = pd.merge(nuevo_df,nuevo_df1,how='inner',on="barrio")
juntos2 = pd.merge(juntos,df_diseño,how='inner',on="barrio")

juntos2
```

## Limpieza dataset resultante

A pesar de que las variables creadas puedan estar lo suficientemente limpias, existe un problema: Medelín no tiene 427 barrios. Al revisar la variable barrios se puede ver que muchos barrios están repetidos en forma de código (Medellín también identifica sus barrios por un código) o con diferencias de mayúsculas y minúsculas, espacios, entre otros, así, luego de hacer la limpieza y el cambio de nombres respectivos para uniformizar todos estos y asumiendo que cuandos dos barrios eran el mismo tenían diferente información por lo que lo procedente era sumarla, resulta el dataframe encontrado en @tbl-barrios-2.

```{python}
#| tbl-cap: Dataframe barrios luego de limpieza
#| label: tbl-barrios-2
#| warning: false

repetidos = [
  ['Asomadera No. 1', 'Asomadera No.1'],
  ['Aures No.2', 'Aures No. 2'],
  ['Aguas frias', 'Aguas Frias'],
  ['Berlin', 'Berlín'],
  ['Bomboná No. 1', 'Bomboná No.1'],
  ['Campo Valdés No.2', 'Campo Valdés No. 2'],
  ['Manrique Central No.1', 'Manrique Central No. 1'],
  ['Manrique Central No.2', 'Manrique Central No. 2'],
  ['Moscú No.2', 'Moscú No. 2'],
  ['Moscú No.1', 'Moscú No. 1'],
  ['Nueva Villa de La Iguaná', 'Nueva Villa de la Iguaná'],
  ['Santo Domingo Savio No.1', 'Santo Domingo Savio No. 1'],
  ['Versalles No.2', 'Versalles No. 2'],
  ['Versalles No.1', 'Versalles No. 1'],
  ['Villa Liliam', 'Villa Lilliam']
 ]

juntos3 = juntos2.copy()

for lista in repetidos:

  tin = juntos3[juntos3['barrio'].isin(lista)]
  tin = tin.sum()

  tin['barrio'] = lista[0]

  juntos3[juntos3['barrio'] == lista[0]] =  tin

  if len(lista) > 1:
    mask = ~juntos3['barrio'].isin(lista[1:])
    juntos3 = juntos3.loc[mask, :]

codigo_barrio = pd.read_csv('codigo_barrio.csv')
codigo_barrio['codigos'] = codigo_barrio['codigos'].apply(lambda x: str(x).zfill(4))

diccionario_barrio = dict(zip(codigo_barrio['codigos'], codigo_barrio['nombres']))


#df['barrio'] = df['barrio'].apply(lambda x:x.replace('\\xC1','Á'))
for word, replacement in diccionario_barrio.items():
  juntos3['barrio'] = juntos3['barrio'].apply(lambda x:x.replace(word,replacement))


import unidecode 

juntos3['barrio junto'] = juntos3['barrio']
juntos3['barrio junto'] = juntos3['barrio junto'].str.replace(' ','')
juntos3['barrio junto'] = juntos3['barrio junto'].str.replace('No.|N°','')
juntos3['barrio junto'] = juntos3['barrio junto'].str.replace('-','')
juntos3['barrio junto'] = juntos3['barrio junto'].str.lower()
juntos3[['barrio junto']] = juntos3[['barrio junto']].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))


juntos3.sort_values(['barrio junto'], inplace=True)
juntos3.reset_index(inplace=True, drop=True)


agrup = {
  'barrio': max,
  'Con muertos': sum,
  'Atropello': sum,
  'Ciclo Ruta': sum,
  'Glorieta': sum,
  'Interseccion': sum,
  'Lote o Predio': sum,
  'Paso Elevado': sum,
  'Paso Inferior': sum,
  'Paso a Nivel': sum,
  'Pontón': sum,
  'Puente': sum,
  'Tramo de via': sum,
  'Tunel': sum,
  'Via peatonal': sum
}
juntos2 = juntos3.groupby('barrio junto').agg(agrup).reset_index(drop=True)

#Revisar si esta linea sale
juntos2 = juntos2.drop(range(0,7)).reset_index(drop = True)
#

#Esta parte se usa para hacer el analisis de correlacion
agrup_anal=agrup.copy()

agrup_anal["Incendio"]=sum
agrup_anal["Otro"]=sum
agrup_anal["Volcamiento"]=sum
agrup_anal["Choque"]=sum
agrup_anal["Caida Ocupante"]=sum

agrup_anal["Con heridos"]=sum
agrup_anal["Solo daños"]=sum

juntos_anal = juntos3.groupby('barrio junto').agg(agrup_anal).reset_index(drop=True)

juntos_anal = juntos_anal.drop(range(0,7)).reset_index(drop = True)
#

juntos_anal
```

Por otra parte en @fig-corr-anal-1 se puede notar la correlación entre las diferentes variables. 

![Matriz de correlacion para barrios a agrupar](corr_for_cluster_anal.png){#fig-corr-anal-1}

De la anterior matriz podemos notar que las variables relacionadas a gravedad_accidente están muy correlacionadas entre sí, por lo que tomaremos sólo una de estas: 'Con muertos', haremos esto mismo para el grupo de variables de 'clase_accidente' pues también están bastante correlacionadas (a excepción de 'incendio' que se obvia por poca cantidad de datos), seleccionando así a 'Atropello' que además fue nuestra variable de interés a predecir anteriormente.

Así terminamos con el dataset: @tbl-barrios-3

```{python}
#| tbl-cap: Dataframe barrios luego de limpieza y analisis de correlacion
#| label: tbl-barrios-3

juntos2
```


## Agrupación

A continuación se precederá a hacer a agrupación para los barrios y zonas especiales de la ciudad de Medellín:

### Cantidad de grupos

Para proceder con el desarrollo del modelo se comenzará escogiendo la cantidad de clusters o grupos en los que se segmentarán los datos. Para ello se hará un análisis de Elbow Curve cuya inercia es obtenida al ajustar los datos al algoritmo K-Means:

![Curva de codo](elbow_curve.png){#fig-elbow-curve}

En @fig-elbow-curve se puede notar que la pendiente de inercia de un cluster a otro comienza a ser menor desde que hay 4 grupos, pero también se podría identificar el codo para 6 grupos. Haciendo un análisis de silueta a partir del algoritmo K-Means y proyectando las observaciones sobre dos ejes obtenemos las gráficas: @fig-silueta-4, @fig-silueta-5 y @fig-silueta-6 para una agrupación respectiva en 4, 5 y 6 grupos para las que se obtuvieron scores respectivos de 0.56945, 0.51939 y 0.51933. Dada la poca diferencia de scores para agrupaciones de 4 y 6 y la notable uniformidad en grupos apreciada en @fig-silueta-6 se tomará 6 como la cantidad de clústeres en la que se agruparán los datos

![Analisis de silueta - 4 grupos](silueta_4.png){#fig-silueta-4}

![Analisis de silueta - 5 grupos](silueta_5.png){#fig-silueta-5}

![Analisis de silueta - 6 grupos](silueta_6.png){#fig-silueta-6}

### Entrenamiento del modelo

Ahora que tenemos la cantidad de clusters seleccionada se procederá a entrenar el modelo. Usaremos un algoritmo de clustering aglomerativo con linkage 'ward' para ello.

Entrenado el modelo en  se podrán notar las observaciones en un plano tridimensional donde se comparan las variables 'Glorieta', 'Con muertos' y 'Paso Elevado' de cada observación y donde el color representa el cluster al que pertenece cada observación.

```{python}
#| label: fig-kmeans-1
#| fig-cap: Observaciones en 3 dimensiones de los clusters

"""
from sklearn.cluster import AgglomerativeClustering
clustering= AgglomerativeClustering(n_clusters=6,linkage="ward")
clustering.fit(juntos2.drop(['barrio'],axis=1))
clustering.labels_

final=juntos2.copy()
final["cluster"]=clustering.labels_

#final.to_csv('final.csv',index=False)

"""

final=pd.read_csv("final.csv")

import plotly.express as px
import plotly.graph_objects as go

fig = px.scatter_3d(final, x="Glorieta", y="Con muertos", z="Paso Elevado",color="cluster",labels='barrio',size_max=0.0001)
fig.show()
```

## Analisis de resultados

Generada la base de datos con los resultados se comienza a hacer el análisis sobre ella. Es de importancia hacer notar que dado el funcionamiento de renderizado de quarto es necesario exportar los resultados generados y usarlos para mantener una constancia en los análisis sin tener problemas por la generación aleatoria de los modelos.

![Histogramas de variables para cada grupo - 1](anal_1.png){#fig-anal-1}

En @fig-anal-1 se pueden observar histogramas para cada grupo generado en cada una de las variables. En primera instancia se puede notar la relevancia que tiene la variable 'Tramo de via' en cada uno de los grupos. Para continuar con un análisis más profundo y notar posibles diferencias entre las demás variables obviaremos esta última:

![Histogramas de variables para cada grupo - 2](anal_2.png){#fig-anal-2}

De @fig-anal-2 se puede observar que:

*   El cluster 0 es el segundo menor en cuanto a accidentalidad en las variables evaluadas.
*   El cluster 1 está compuesto por los barrios de mayor accidentalidad por atropellos y accidentes ocurridos en intersecciones, lotes o predios.
*   El cluster 2 es el tercer menor grupo de barrios en cuanto a accidentalidad en las variables evaludas.
*   El cluster 3 está compuesto por el grupo de barrios cuya accidentalidad es la segunda mayor en comparación a los otros.
*   El cluster 4 es el grupo de barrios con menor accidentalidad en todas las variables.
*   El cluster 5 es el tercer mayor en cuanto a accidentalidad.
*   En general, en todos los clusters la cantidad de accidentes ocurridos en paso inferior, paso a nivel, portón, puente, túnel y vía peatonal son muy pocos por lo que no se considera relevante profundizar en estos resultados.

Además de esto también es posible centrarse en la variable 'Tramo de via' y algunos de sus similares:

![Histogramas de variables para cada grupo - 3](anal_3.png){#fig-anal-3}

De estas gráficas se puede ver que:

*   El orden en cuanto a accidentalidad se mantiene entre los grupos, siendo el de mayor media de accidentes el grupo 1 y el de menor media el grupo 4.
*   Los accidentes en tramo de vía son con gran diferencia los de mayor ocurrencia y los accidentes con muertos o en ciclorruta los de menor.

Pasando a analizar las estadísticas descriptivas para el caso de Atropellos como accidentes tenemos:

```{python}
#| tbl-cap: Analisis descriptivo sobre los grupos y la variable Atropello
#| label: tbl-anal-descr



anal_descr=final[['cluster', "Atropello"]].groupby('cluster').describe()

anal_descr
```

Así, se puede notar que:

*   El cluster 0 es el de mayor número de barrios (132) y aún así es el segundo menor media de accidentes.
*   A pesar de que el cluster 1 solo tiene 8 barrios, es el de mayor media de accidentalidad por lo que se requiere especial atención a estos barrios para realizar intervenciones con el fin de reducir ésta medida.
*   El cluster 2 a pesar de que tiene aproximadamente 8 veces más barrios que el cluster 1 (tiene 63 exactamente), es el tercer menor grupo en cuanto a accidentalidad.
*   El cluster 3, similar que el cluster 1 es el segundo grupo con menor cantidad de barrios (13) sin embargo, es el segundo mayor en cuanto a accidentalidad media por lo que también requiere especial atención con el fin de implementar medidas para reducir esto.
*   El cluster 4 es el de menor accidentalidad media a pese a ser el segundo en cuanto a cantidad de barrios (106).
*   El cluster 5 similar a lo que ocurre en los clusters 1 y 3 es de los mayores en accidentalidad a pesar de solo tener 13 barrios.
*   En general se identifica que los clusters con mayor cantidad de barrios, son los de menor accidentalidad y los de menor cantidad de barrios los de mayor accidentalidad.

En general se pueden dividir los clusters en 2:

*   El primer grupo es el de mayor accidentalidad, compuesto por los clusters 1, 3 y 5 (34 barrios en total) los cuales deben ser intervenidos con urgencia.
*   El segundo grupo es el de menor accidentalidad, compuesto por los clusters 0, 2 y 4 (301 barrios en total) 

---
nocite: |
  @*
---

# Referencias

::: {#refs}
:::